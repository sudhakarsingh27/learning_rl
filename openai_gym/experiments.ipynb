{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b8c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548f63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 01:49:35.929098: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 51050315776\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.372111    0.2642311  -0.18252774 -0.7368198  -0.44030386 -0.15214427\n",
      " -0.6713536  -0.59086424  0.73168874  0.56730247]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9adef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76c224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18 ms ± 38 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1317eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [2, 3]\n",
    "param_scale = 0.1\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "n_targets = 3\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e57386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DeviceArray([[-0.01454695,  0.00973438],\n",
       "               [-0.00217348,  0.00691426],\n",
       "               [-0.01011404,  0.00400959]], dtype=float32),\n",
       "  DeviceArray([-0.00066073,  0.00166766,  0.01178003], dtype=float32))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668a7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def q_value(params, x):\n",
    "    # per-example predictions\n",
    "    activations = x\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "  \n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cadae544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random.normal(random.PRNGKey(1), (3,))\n",
    "\n",
    "int(jnp.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9f115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-1.0965854, -1.101934 , -1.0973257], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2b4364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(np.random.random(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d65ff2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot(x, k, dtype=jnp.float32):\n",
    "#     \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "#     return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "# def accuracy(params, images, targets):\n",
    "#     target_class = jnp.argmax(targets, axis=1)\n",
    "#     predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "#     return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "\n",
    "#         delta = learning*(reward + \n",
    "#                          discount*np.max(Q[state2_adj[0], \n",
    "#                                            state2_adj[1]]) - \n",
    "#                          Q[state_adj[0], state_adj[1],action])\n",
    "#         Q[state_adj[0], state_adj[1],action] += delta\n",
    "\n",
    "def loss(params, next_state, reward, cur_state_rewards, action, discount, learning):\n",
    "    future_rewards = q_value(params, jnp.array(next_state[0], next_state[1]))\n",
    "    delta = jnp.array([0,0,0])\n",
    "    delta[action] = learning*(reward + discount*jnp.max(future_rewards) - \n",
    "                                 cut_state_rewards[action])\n",
    "    return delta\n",
    "\n",
    "def update(params, next_state, reward, cur_state_rewards, action, discount, learning):\n",
    "    grads = grad(loss)(params, next_state, reward, cur_state_rewards, action, discount, learning)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "def terminal_state_loss(reward, action, cur_state_rewards):\n",
    "    delta = jnp.array([0,0,0])\n",
    "    delta[action] = reward - cur_state_rewards[action]\n",
    "    return delta\n",
    "    \n",
    "def update_terminal_state(params, reward, action, cur_state_rewards):\n",
    "    grads = grad(terminal_state_loss)(reward, action, cur_state_rewards)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645a02a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_279307/536518541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#     # Render environment for last five episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     if i >= (episodes - 20):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#         env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "while done != True:   \n",
    "#     # Render environment for last five episodes\n",
    "#     if i >= (episodes - 20):\n",
    "#         env.render()\n",
    "\n",
    "    # In any case, get the rewards for current state\n",
    "    cur_state_rewards = q_value(params, jnp.array([cur_state[0], cur_state[1]]))\n",
    "    \n",
    "    # Determine next action - epsilon greedy strategy\n",
    "    if np.random.random() < 1 - epsilon:\n",
    "        action = np.argmax(cur_state_rewards) \n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "    # Get next state and reward\n",
    "    next_state, reward, done, info = env.step(action) \n",
    "\n",
    "    #Allow for terminal states\n",
    "    if done and next_state[0] >= 0.5:\n",
    "        \n",
    "        params = update_terminal_state(params, reward, action, cur_state_rewards)\n",
    "        \n",
    "        ## UPDATE NN based on q_value\n",
    "\n",
    "    # Adjust Q value for current state\n",
    "    else:\n",
    "        params = update(params, next_state, reward, cur_state_rewards, action, discount, learning)\n",
    "\n",
    "    # Update variables\n",
    "    tot_reward += reward\n",
    "    cur_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac25460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01700249, -0.03022148, -0.03244098,  0.02547737], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e16e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low, env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202843df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36b98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to apply Deep Q networks for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5916c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.5664178 ,  0.00133015], dtype=float32), -1.0, False, {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05c4c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1face8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gym import wrappers\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671ed4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c272bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (3.18.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (0.14.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (2.26.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (21.2.0)\n",
      "Requirement already satisfied: numpy in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from tensorflow-datasets) (1.19.5)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sudhakars/anaconda3/envs/gym/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.2)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=b600d1911cfc051bd4fb5e18d08e5997a26229c43b1d9e8fb8fde4d68b1c8f0e\n",
      "  Stored in directory: /home/sudhakars/.cache/pip/wheels/2f/a0/d3/4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=f133545c7e377e3491b7a502a87b77486911c3557d9782a90da62c20ecc5ad81\n",
      "  Stored in directory: /home/sudhakars/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built future promise\n",
      "Installing collected packages: googleapis-common-protos, absl-py, tqdm, tensorflow-metadata, promise, future, dill, tensorflow-datasets\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.14.1\n",
      "    Uninstalling absl-py-0.14.1:\n",
      "      Successfully uninstalled absl-py-0.14.1\n",
      "Successfully installed absl-py-0.12.0 dill-0.3.4 future-0.18.2 googleapis-common-protos-1.53.0 promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.2.0 tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a591f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  preds = batched_predict(params, images)\n",
    "  return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3d260f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e6c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_dir = '/tmp/tfds'\n",
    "\n",
    "# Fetch full datasets for evaluation\n",
    "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
    "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cd448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "param_scale = 0.1\n",
    "step_size = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1acf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(batch_size, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ae2f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Incompatible shapes for dot: got (512, 784) and (128, 784).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274900/1989939234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_274900/3874248894.py\u001b[0m in \u001b[0;36mq_value\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   4762\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4763\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4764\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4766\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mb_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.9/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    667\u001b[0m                        precision=precision, preferred_element_type=preferred_element_type)\n\u001b[1;32m    668\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m     raise TypeError(\"Incompatible shapes for dot: got {} and {}.\".format(\n\u001b[0m\u001b[1;32m    670\u001b[0m         lhs.shape, rhs.shape))\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Incompatible shapes for dot: got (512, 784) and (128, 784)."
     ]
    }
   ],
   "source": [
    "q_value(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d11336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 00:43:20.907532: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'update' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274900/919371054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'update' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_train_batches():\n",
    "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
    "  # You can build up an arbitrary tf.data input pipeline\n",
    "  ds = ds.batch(batch_size).prefetch(1)\n",
    "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "  return tfds.as_numpy(ds)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in get_train_batches():\n",
    "    x = jnp.reshape(x, (len(x), num_pixels))\n",
    "    y = one_hot(y, num_labels)\n",
    "    params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060825ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
